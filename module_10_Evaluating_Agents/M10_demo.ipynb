{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4fc9f67",
   "metadata": {},
   "source": [
    "Agent Evaluation\n",
    "\n",
    "This notebook demonstrates different agent evaluation approaches by creating test scenarios and evaluation criteria.\n",
    "\n",
    "## What we'll learn:\n",
    "\n",
    "- Metrics\n",
    "- Test Case\n",
    "- Evaluator\n",
    "- Approaches\n",
    "    - black box, \n",
    "    - single step, \n",
    "    - trajectory\n",
    "- Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c08fcd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3448288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f655eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lib.agents import Agent, AgentState\n",
    "from lib.state_machine import Run\n",
    "from lib.messages import AIMessage\n",
    "from lib.tooling import tool\n",
    "from lib.evaluation import TestCase, AgentEvaluator, EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4bd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe324c7",
   "metadata": {},
   "source": [
    "## Your Custom Tools\n",
    "\n",
    "Design and implement tools that will showcase different aspects of agent behavior. Consider creating tools that:\n",
    "\n",
    "- Require multi-step reasoning\n",
    "- Handle different data types and formats\n",
    "- Interact with external services or APIs\n",
    "- Process complex inputs\n",
    "- Have potential failure modes to test error handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_games(num_games: int = 1, top: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Returns the top or bottom N games with highest or lowest scores.    \n",
    "    args:\n",
    "        num_games (int): Number of games to return (default is 1)\n",
    "        top (bool): If True, return top games, otherwise return bottom (default is True)\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98},\n",
    "        {\"Game\": \"Super Mario Odyssey\", \"Platform\": \"Switch\", \"Score\": 97},\n",
    "        {\"Game\": \"Metroid Prime\", \"Platform\": \"GameCube\", \"Score\": 97},\n",
    "        {\"Game\": \"Super Smash Bros. Brawl\", \"Platform\": \"Wii\", \"Score\": 93},\n",
    "        {\"Game\": \"Mario Kart 8 Deluxe\", \"Platform\": \"Switch\", \"Score\": 92},\n",
    "        {\"Game\": \"Fire Emblem: Awakening\", \"Platform\": \"3DS\", \"Score\": 92},\n",
    "        {\"Game\": \"Donkey Kong Country Returns\", \"Platform\": \"Wii\", \"Score\": 87},\n",
    "        {\"Game\": \"Luigi's Mansion 3\", \"Platform\": \"Switch\", \"Score\": 86},\n",
    "        {\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85},\n",
    "        {\"Game\": \"Animal Crossing: New Leaf\", \"Platform\": \"3DS\", \"Score\": 88}\n",
    "    ]\n",
    "    # Sort the games list by Score\n",
    "    sorted_games = sorted(data, key=lambda x: x['Score'], reverse=top)\n",
    "    return sorted_games[:num_games]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0b328",
   "metadata": {},
   "source": [
    "## Develop your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf079568",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    tools=[get_games],\n",
    "    instructions=\"You can bring insights about a game dataset based on users questions\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c62c60",
   "metadata": {},
   "source": [
    "## Design Your Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abd8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AgentEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dd713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"game_query_1\",\n",
    "        description=\"Find the best game in the dataset\",\n",
    "        user_query=\"What's the best game in the dataset?\",\n",
    "        expected_tools=[\"get_games\"],\n",
    "        reference_answer=\"The Legend of Zelda: Breath of the Wild with score 98\",\n",
    "        max_steps=4\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53118ad7",
   "metadata": {},
   "source": [
    "## Run Your Evaluation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08152474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Test Case: game_query_1 ===\n",
      "Description: Find the best game in the dataset\n",
      "Query: What's the best game in the dataset?\n",
      "\n",
      "Workflow:\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "\n",
      "--- Black Box (Final Response) Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Task Completed: True\n",
      "Feedback: The agent response successfully identifies the best game in the dataset, providing the name and score, which fully answers the user's query. The format is appropriate as it clearly presents the information. Additionally, the response follows implicit instructions by providing a definitive answer to the question asked.\n",
      "\n",
      "--- Single Step Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Correct Tool Selected: True\n",
      "Feedback: Selected tools: ['get_games'], Expected: ['get_games']\n",
      "\n",
      "--- Trajectory Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Steps Taken: 4\n",
      "Total Tokens: 333\n",
      "Execution Time: 3.20s\n",
      "Estimated Cost: $0.000125\n",
      "Feedback: Trajectory: 4 steps, Tools used: ['get_games'], Expected: ['get_games']\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    print(f\"\\n=== Evaluating Test Case: {test_case.id} ===\")\n",
    "    print(f\"Description: {test_case.description}\")\n",
    "    print(f\"Query: {test_case.user_query}\")\n",
    "    \n",
    "    # Run the agent\n",
    "    start_time = time.time()\n",
    "    print(\"\\nWorkflow:\")\n",
    "    agent.memory.reset()\n",
    "    run_object:Run = agent.invoke(test_case.user_query)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Get final state and response\n",
    "    final_state:AgentState = run_object.get_final_state()\n",
    "    if final_state and final_state.get(\"messages\"):\n",
    "        # Find the last AI message as the final response\n",
    "        final_response = \"\"\n",
    "        for msg in reversed(final_state[\"messages\"]):\n",
    "            if isinstance(msg, AIMessage) and msg.content:\n",
    "                final_response = msg.content\n",
    "                break\n",
    "        \n",
    "        total_tokens = final_state.get(\"total_tokens\", 0)\n",
    "        \n",
    "        # Evaluate using all three methods\n",
    "        print(\"\\n--- Black Box (Final Response) Evaluation ---\")\n",
    "        black_box_eval:EvaluationResult = evaluator.evaluate_final_response(\n",
    "            test_case, final_response, execution_time, total_tokens\n",
    "        )\n",
    "        print(f\"Overall Score: {black_box_eval.overall_score:.2f}\")\n",
    "        print(f\"Task Completed: {black_box_eval.task_completion.task_completed}\")\n",
    "        print(f\"Feedback: {black_box_eval.feedback}\")\n",
    "        \n",
    "        print(\"\\n--- Single Step Evaluation ---\")\n",
    "        step_eval:EvaluationResult = evaluator.evaluate_single_step(\n",
    "            final_state[\"messages\"], test_case.expected_tools\n",
    "        )\n",
    "        print(f\"Overall Score: {step_eval.overall_score:.2f}\")\n",
    "        print(f\"Correct Tool Selected: {step_eval.tool_interaction.correct_tool_selected}\")\n",
    "        print(f\"Feedback: {step_eval.feedback}\")\n",
    "        \n",
    "        print(\"\\n--- Trajectory Evaluation ---\")\n",
    "        traj_eval:EvaluationResult = evaluator.evaluate_trajectory(test_case, run_object)\n",
    "        print(f\"Overall Score: {traj_eval.overall_score:.2f}\")\n",
    "        print(f\"Steps Taken: {traj_eval.task_completion.steps_taken}\")\n",
    "        print(f\"Total Tokens: {traj_eval.system_metrics.total_tokens}\")\n",
    "        print(f\"Execution Time: {traj_eval.system_metrics.execution_time:.2f}s\")\n",
    "        print(f\"Estimated Cost: ${traj_eval.system_metrics.cost_estimate:.6f}\")\n",
    "        print(f\"Feedback: {traj_eval.feedback}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: No final state or messages found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe7692",
   "metadata": {},
   "source": [
    "## Reflection and Iteration\n",
    "\n",
    "Based on your evaluation results:\n",
    "\n",
    "1. **Identify Patterns**: What trends do you see in your agent's performance?\n",
    "2. **Spot Weaknesses**: Where does your agent struggle most?\n",
    "3. **Recognize Strengths**: What does your agent do particularly well?\n",
    "4. **Iterate**: Modify your tools, test cases, or agent configuration and re-evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d99eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0 (main, Dec  3 2024, 02:24:14) [GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
