{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9baf30a2",
   "metadata": {},
   "source": [
    "# [SOLUTION] Exercise - Building a Multi-Step State Machine Agent\n",
    "\n",
    "In this exercise, you will build an agent that manages a multi-step workflow using a state machine. You’ll define a custom state schema, implement step logic, connect steps (including conditional routing and loops), and run the workflow to process user input through several transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb9056",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You have learned how to use a state machine to manage workflow steps and transitions. Now, your challenge is to:\n",
    "\n",
    "- Define a state schema with multiple fields (e.g., user_query, instructions, messages, current_tool_calls).\n",
    "- Implement at least three step functions:\n",
    "    - Prepare Messages: Assemble the conversation history and any required context for the LLM.\n",
    "    - LLM: Call the language model to generate a response or tool call.\n",
    "    - Tools: Execute any required tool calls and update the state with results.\n",
    "- Connect steps to form a workflow, including:\n",
    "    - Entrypoint and Termination steps to start and end the workflow.\n",
    "    - Conditional routing: If the LLM response includes tool calls, route to the Tools step; otherwise, proceed to Termination.\n",
    "    - Looping: After executing tools, return to the LLM step to continue the workflow until there are no more tool calls.\n",
    "- Run your state machine with a sample input and inspect the state transitions and snapshots to understand how your agent processes a task step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0e231",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42ccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional, Union\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run\n",
    "from lib.llm import LLM\n",
    "from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage\n",
    "from lib.tooling import Tool, ToolCall, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a8686cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41de723",
   "metadata": {},
   "source": [
    "## Define a State Schema\n",
    "\n",
    "Create a TypedDict to represent the agent’s state, including fields for the user query, instructions, message history, and any pending tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99605c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_query: str  # The current user query being processed\n",
    "    instructions: str  # System instructions for the agent\n",
    "    messages: List[dict]  # List of conversation messages\n",
    "    current_tool_calls: Optional[List[ToolCall]]  # Current pending tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da75109",
   "metadata": {},
   "source": [
    "## Define the Tools you will use\n",
    "\n",
    "Feel free to modify to add any tool you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d171e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_games(num_games:int=1, top:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Returns the top or bottom N games with highest or lowest scores.    \n",
    "    args:\n",
    "        num_games (int): Number of games to return (default is 1)\n",
    "        top (bool): If True, return top games, otherwise return bottom (default is True)\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98},\n",
    "        {\"Game\": \"Super Mario Odyssey\", \"Platform\": \"Switch\", \"Score\": 97},\n",
    "        {\"Game\": \"Metroid Prime\", \"Platform\": \"GameCube\", \"Score\": 97},\n",
    "        {\"Game\": \"Super Smash Bros. Brawl\", \"Platform\": \"Wii\", \"Score\": 93},\n",
    "        {\"Game\": \"Mario Kart 8 Deluxe\", \"Platform\": \"Switch\", \"Score\": 92},\n",
    "        {\"Game\": \"Fire Emblem: Awakening\", \"Platform\": \"3DS\", \"Score\": 92},\n",
    "        {\"Game\": \"Donkey Kong Country Returns\", \"Platform\": \"Wii\", \"Score\": 87},\n",
    "        {\"Game\": \"Luigi's Mansion 3\", \"Platform\": \"Switch\", \"Score\": 86},\n",
    "        {\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85},\n",
    "        {\"Game\": \"Animal Crossing: New Leaf\", \"Platform\": \"3DS\", \"Score\": 88}\n",
    "    ]\n",
    "    # Sort the games list by Score\n",
    "    # If top is True, descending order\n",
    "    sorted_games = sorted(data, key=lambda x: x['Score'], reverse=top)\n",
    "    \n",
    "    # Return the N games\n",
    "    return sorted_games[:num_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07c2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_games]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052be6c2",
   "metadata": {},
   "source": [
    "## Create the Steps\n",
    "\n",
    "Write functions for each step in your workflow:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd86828",
   "metadata": {},
   "source": [
    "**Prepare Messages**: Build the message list for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6341e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=state[\"instructions\"]),\n",
    "        UserMessage(content=state[\"user_query\"])\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": messages\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ce1ea",
   "metadata": {},
   "source": [
    "**LLM Step**: Call the language model and check for tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbf3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = LLM(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "    # Create AI message with content and tool calls\n",
    "    ai_message = AIMessage(content=response.content, tool_calls=tool_calls)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [ai_message],\n",
    "        \"current_tool_calls\": tool_calls\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288ecd4",
   "metadata": {},
   "source": [
    "**Tool Step**: Execute any tool calls and update the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32124e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "    tool_calls = state[\"current_tool_calls\"] or []\n",
    "    tool_messages = []\n",
    "    \n",
    "    for call in tool_calls:\n",
    "        # Access tool call data correctly\n",
    "        function_name = call.function.name\n",
    "        function_args = json.loads(call.function.arguments)\n",
    "        tool_call_id = call.id\n",
    "        # Find the matching tool\n",
    "        tool = next((t for t in tools if t.name == function_name), None)\n",
    "        if tool:\n",
    "            result = tool(**function_args)\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(result), \n",
    "                    tool_call_id=tool_call_id, \n",
    "                    name=function_name, \n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Clear tool calls and add results to messages\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + tool_messages,\n",
    "        \"current_tool_calls\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaddee",
   "metadata": {},
   "source": [
    "## Build and Connect the State Machine\n",
    "\n",
    "Add your steps to the state machine, and connect them with transitions. Use conditional routing to decide whether to call tools or terminate, and loop as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58ca0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateMachine[AgentState](AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d52f8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steps\n",
    "entry = EntryPoint[AgentState]()\n",
    "message_prep = Step[AgentState](\"message_prep\", prepare_messages_step)\n",
    "llm_processor = Step[AgentState](\"llm_processor\", llm_step)\n",
    "tool_executor = Step[AgentState](\"tool_executor\", tool_step)\n",
    "termination = Termination[AgentState]()\n",
    "        \n",
    "workflow.add_steps(\n",
    "    [\n",
    "        entry, \n",
    "        message_prep, \n",
    "        llm_processor, \n",
    "        tool_executor, \n",
    "        termination\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae5a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transitions\n",
    "workflow.connect(entry, message_prep)\n",
    "workflow.connect(message_prep, llm_processor)\n",
    "\n",
    "# Transition based on whether there are tool calls\n",
    "def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "    \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "    if state.get(\"current_tool_calls\"):\n",
    "        return tool_executor\n",
    "    return termination\n",
    "\n",
    "# Routing: If tool calls -> tool_executor\n",
    "workflow.connect(\n",
    "    source=llm_processor, \n",
    "    targets=[tool_executor, termination], \n",
    "    condition=check_tool_calls\n",
    ")\n",
    "\n",
    "# Looping: Go back to llm after tool execution\n",
    "workflow.connect(\n",
    "    source=tool_executor, \n",
    "    targets=llm_processor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffeb8b6",
   "metadata": {},
   "source": [
    "## Run the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df4facbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state: AgentState = {\n",
    "    \"user_query\": \"What's the best game in the dataset?\",\n",
    "    \"instructions\": \"You can bring insights about a game dataset based on users questions\",\n",
    "    \"messages\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8174c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m run_object = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/module_03_State_Management/lib/state_machine.py:206\u001b[39m, in \u001b[36mStateMachine.run\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Replace state entirely\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m state = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate_schema\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, EntryPoint):\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[StateMachine] Starting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_step_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/module_03_State_Management/lib/state_machine.py:22\u001b[39m, in \u001b[36mStep.run\u001b[39m\u001b[34m(self, state, state_schema)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: StateSchema, state_schema: Type[StateSchema]) -> StateSchema:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Get expected fields from the TypedDict\u001b[39;00m\n\u001b[32m     24\u001b[39m     expected_fields = get_type_hints(state_schema)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mllm_step\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Step logic: Process the current state through the LLM\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m response = llm.invoke(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     12\u001b[39m tool_calls = response.tool_calls \u001b[38;5;28;01mif\u001b[39;00m response.tool_calls \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/module_03_State_Management/lib/llm.py:23\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, temperature, tools, api_key)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.temperature = temperature\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.client = OpenAI(api_key=api_key) \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://openai.vocareum.com/v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mself\u001b[39m.tools: Dict[\u001b[38;5;28mstr\u001b[39m, Tool] = {\n\u001b[32m     28\u001b[39m     tool.name: tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m (tools \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m     29\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.13/site-packages/openai/_client.py:116\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    114\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "run_object = workflow.run(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f2c752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'),\n",
       " UserMessage(role='user', content=\"What's the best game in the dataset?\"),\n",
       " AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yxCOOsxBkcbjOhJIit1Akdhw', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]),\n",
       " ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_yxCOOsxBkcbjOhJIit1Akdhw', name='get_games'),\n",
       " AIMessage(role='assistant', content='The best game in the dataset is **The Legend of Zelda: Breath of the Wild** for the Switch, with a score of **98**.', tool_calls=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_object.get_final_state()[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89629e4a",
   "metadata": {},
   "source": [
    "## Optional \n",
    "\n",
    "Create an Agent class to encapsulate State Machine logic. Then try adding more tools, and experiment with different user queries to see how the workflow adapts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e7d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 instructions: str, \n",
    "                 tools: List[Tool] = None,\n",
    "                 temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize an Agent instance\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name/identifier of the LLM model to use\n",
    "            instructions: System instructions for the agent\n",
    "            tools: Optional list of tools available to the agent\n",
    "            temperature: Temperature parameter for LLM (default: 0.7)\n",
    "        \"\"\"\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools if tools else []\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "                \n",
    "        # Initialize state machine\n",
    "        self.workflow = self._create_state_machine()\n",
    "\n",
    "    def _prepare_messages_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=state[\"instructions\"]),\n",
    "            UserMessage(content=state[\"user_query\"])\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    def _llm_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = LLM(\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "        # Create AI message with content and tool calls\n",
    "        ai_message = AIMessage(content=response.content, tool_calls=tool_calls)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"current_tool_calls\": tool_calls\n",
    "        }\n",
    "\n",
    "    def _tool_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "        tool_calls = state[\"current_tool_calls\"] or []\n",
    "        tool_messages = []\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            # Access tool call data correctly\n",
    "            function_name = call.function.name\n",
    "            function_args = json.loads(call.function.arguments)\n",
    "            tool_call_id = call.id\n",
    "            # Find the matching tool\n",
    "            tool = next((t for t in self.tools if t.name == function_name), None)\n",
    "            if tool:\n",
    "                result = tool(**function_args)\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=json.dumps(result), \n",
    "                        tool_call_id=tool_call_id, \n",
    "                        name=function_name, \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Clear tool calls and add results to messages\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + tool_messages,\n",
    "            \"current_tool_calls\": None\n",
    "        }\n",
    "\n",
    "    def _create_state_machine(self) -> StateMachine[AgentState]:\n",
    "        \"\"\"Create the internal state machine for the agent\"\"\"\n",
    "        machine = StateMachine[AgentState](AgentState)\n",
    "        \n",
    "        # Create steps\n",
    "        entry = EntryPoint[AgentState]()\n",
    "        message_prep = Step[AgentState](\"message_prep\", self._prepare_messages_step)\n",
    "        llm_processor = Step[AgentState](\"llm_processor\", self._llm_step)\n",
    "        tool_executor = Step[AgentState](\"tool_executor\", self._tool_step)\n",
    "        termination = Termination[AgentState]()\n",
    "        \n",
    "        machine.add_steps([entry, message_prep, llm_processor, tool_executor, termination])\n",
    "        \n",
    "        # Add transitions\n",
    "        machine.connect(entry, message_prep)\n",
    "        machine.connect(message_prep, llm_processor)\n",
    "        \n",
    "        # Transition based on whether there are tool calls\n",
    "        def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "            \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "            if state.get(\"current_tool_calls\"):\n",
    "                return tool_executor\n",
    "            return termination\n",
    "        \n",
    "        machine.connect(llm_processor, [tool_executor, termination], check_tool_calls)\n",
    "        machine.connect(tool_executor, llm_processor)  # Go back to llm after tool execution\n",
    "        \n",
    "        return machine\n",
    "\n",
    "    def invoke(self, query: str) -> Run:\n",
    "        \"\"\"\n",
    "        Run the agent on a query\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to process\n",
    "            \n",
    "        Returns:\n",
    "            The final run object after processing\n",
    "        \"\"\"\n",
    "\n",
    "        initial_state: AgentState = {\n",
    "            \"user_query\": query,\n",
    "            \"instructions\": self.instructions,\n",
    "            \"messages\": [],\n",
    "        }\n",
    "\n",
    "        run_object = self.workflow.run(initial_state)\n",
    "\n",
    "        return run_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c738d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def power(base:float, exponent:float):\n",
    "    \"\"\"Exponentatiation: base to the power of exponent\"\"\"\n",
    "    \n",
    "    return base ** exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887bda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(number_a:float, number_b:float):\n",
    "    \"\"\"Multiplication: number_a times number_b\"\"\"\n",
    "    \n",
    "    return number_a * number_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a1da1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [power, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "356eba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    instructions=(\n",
    "        \"You're an AI Agent very good with math operations \"\n",
    "        \"You can answer multistep questions by sequentially calling functions. \"\n",
    "        \"You follow a pattern of of Thought and Action. \"\n",
    "        \"Create a plan of execution: \"\n",
    "        \"- Use Thought to describe your thoughts about the question you have been asked. \"\n",
    "        \"- Use Action to specify one of the tools available to you. if you don't have a tool available, you can respond directly.\"\n",
    "        \"When you think it's over, return the answer \"\n",
    "        \"Never try to respond directly if the question needs a tool. \"\n",
    "        \"But if you don't have a tool available, you can respond directly. \"\n",
    "        f\"The actions you have are the Tools: {tools}. \\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84dffc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n"
     ]
    }
   ],
   "source": [
    "run_object = math_agent.invoke(\n",
    "    query=\"What's 3 to the power of 2? Take the result, then multiply it by 5.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d29595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(role='system', content=\"You're an AI Agent very good with math operations You can answer multistep questions by sequentially calling functions. You follow a pattern of of Thought and Action. Create a plan of execution: - Use Thought to describe your thoughts about the question you have been asked. - Use Action to specify one of the tools available to you. if you don't have a tool available, you can respond directly.When you think it's over, return the answer Never try to respond directly if the question needs a tool. But if you don't have a tool available, you can respond directly. The actions you have are the Tools: [<Tool name=power params=['base', 'exponent']>, <Tool name=multiply params=['number_a', 'number_b']>]. \\n\"),\n",
       " UserMessage(role='user', content=\"What's 3 to the power of 2? Take the result, then multiply it by 5.\"),\n",
       " AIMessage(role='assistant', content='Thought: First, I need to calculate \\\\(3\\\\) to the power of \\\\(2\\\\). Then, I will take that result and multiply it by \\\\(5\\\\). \\n\\nAction: I will use the power tool to calculate \\\\(3\\\\) to the power of \\\\(2\\\\).', tool_calls=[ChatCompletionMessageToolCall(id='call_KbjdkuHcTpzDAGuVis0dUfNz', function=Function(arguments='{\"base\":3,\"exponent\":2}', name='power'), type='function')]),\n",
       " ToolMessage(role='tool', content='9', tool_call_id='call_KbjdkuHcTpzDAGuVis0dUfNz', name='power'),\n",
       " AIMessage(role='assistant', content='Thought: The result of \\\\(3\\\\) to the power of \\\\(2\\\\) is \\\\(9\\\\). Now, I will multiply this result by \\\\(5\\\\).\\n\\nAction: I will use the multiply tool to calculate \\\\(9 \\\\times 5\\\\).', tool_calls=[ChatCompletionMessageToolCall(id='call_L7DRagsmJIPuSNB8vYdjWslN', function=Function(arguments='{\"number_a\":9,\"number_b\":5}', name='multiply'), type='function')]),\n",
       " ToolMessage(role='tool', content='45', tool_call_id='call_L7DRagsmJIPuSNB8vYdjWslN', name='multiply'),\n",
       " AIMessage(role='assistant', content='The final answer is \\\\(45\\\\).', tool_calls=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_object.get_final_state()[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceddaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
